{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-17T11:43:13.724249Z",
     "start_time": "2025-09-17T11:43:13.061576Z"
    }
   },
   "source": [
    "#QUESTION 1 & 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('housing/housing.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values in total_bedrooms\n",
    "# Since total_bedrooms is correlated with households and total_rooms,\n",
    "# we can use median imputation by grouping similar properties\n",
    "\n",
    "# Calculate median bedrooms per household for each ocean_proximity category\n",
    "bedroom_median_by_location = df.groupby('ocean_proximity')['total_bedrooms'].median()\n",
    "\n",
    "# Impute missing values\n",
    "df['total_bedrooms'] = df.apply(\n",
    "    lambda row: bedroom_median_by_location[row['ocean_proximity']]\n",
    "    if pd.isna(row['total_bedrooms'])\n",
    "    else row['total_bedrooms'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Verify no more missing values\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('housing/cleaned_housing_data.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved as 'cleaned_housing_data.csv'\")\n",
    "# Alternative approach: Simple median imputation (if you prefer)\n",
    "# df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "longitude               0\n",
      "latitude                0\n",
      "housing_median_age      0\n",
      "total_rooms             0\n",
      "total_bedrooms        207\n",
      "population              0\n",
      "households              0\n",
      "median_income           0\n",
      "median_house_value      0\n",
      "ocean_proximity         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "longitude             0\n",
      "latitude              0\n",
      "housing_median_age    0\n",
      "total_rooms           0\n",
      "total_bedrooms        0\n",
      "population            0\n",
      "households            0\n",
      "median_income         0\n",
      "median_house_value    0\n",
      "ocean_proximity       0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset saved as 'cleaned_housing_data.csv'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:07:21.799055Z",
     "start_time": "2025-09-17T12:07:21.627184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 4, HANDLE THE NON-NUMERICAL FIELD\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# One-hot encoding with pandas\n",
    "ohe_df = pd.get_dummies(df['ocean_proximity'], prefix='ocean')\n",
    "\n",
    "# Concatenate with original dataframe\n",
    "df_encoded = pd.concat([df.drop('ocean_proximity', axis=1), ohe_df], axis=1)\n",
    "\n",
    "print(\"One-Hot Encoding Results:\")\n",
    "print(ohe_df.head())\n",
    "print(f\"\\nNew columns: {list(ohe_df.columns)}\")"
   ],
   "id": "3b7a4ae6f261d697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding Results:\n",
      "   ocean_<1H OCEAN  ocean_INLAND  ocean_ISLAND  ocean_NEAR BAY  \\\n",
      "0            False         False         False            True   \n",
      "1            False         False         False            True   \n",
      "2            False         False         False            True   \n",
      "3            False         False         False            True   \n",
      "4            False         False         False            True   \n",
      "\n",
      "   ocean_NEAR OCEAN  \n",
      "0             False  \n",
      "1             False  \n",
      "2             False  \n",
      "3             False  \n",
      "4             False  \n",
      "\n",
      "New columns: ['ocean_<1H OCEAN', 'ocean_INLAND', 'ocean_ISLAND', 'ocean_NEAR BAY', 'ocean_NEAR OCEAN']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:26:45.754468Z",
     "start_time": "2025-09-17T12:26:45.379646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 5 :: Use Scikit-learn Pipeline class for transformation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Custom transformer for your specific bedroom imputation strategy\n",
    "class BedroomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.bedroom_medians_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate median bedrooms by ocean_proximity during fitting\n",
    "        self.bedroom_medians_ = X.groupby('ocean_proximity')['total_bedrooms'].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # Apply the same imputation logic you used\n",
    "        X_copy['total_bedrooms'] = X_copy.apply(\n",
    "            lambda row: self.bedroom_medians_[row['ocean_proximity']]\n",
    "            if pd.isna(row['total_bedrooms'])\n",
    "            else row['total_bedrooms'],\n",
    "            axis=1\n",
    "        )\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# Define numeric and categorical features\n",
    "numeric_features = ['longitude', 'latitude', 'housing_median_age',\n",
    "                    'total_rooms', 'total_bedrooms', 'population',\n",
    "                    'households', 'median_income']\n",
    "categorical_features = ['ocean_proximity']\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    # Your custom bedroom imputation\n",
    "    ('bedroom_imputer', BedroomImputer()),\n",
    "\n",
    "    # Column transformer for different data types\n",
    "    ('column_transformer', ColumnTransformer([\n",
    "        # Numeric features: scale them\n",
    "        ('numeric', StandardScaler(), numeric_features),\n",
    "\n",
    "        # Categorical features: one-hot encode\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "         categorical_features)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Example of how to use the pipeline:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('housing/housing.csv')\n",
    "\n",
    "    # Split data (you'll want to do this properly)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train_processed = preprocessing_pipeline.fit_transform(train_df)\n",
    "\n",
    "    # Transform the test data (using learned parameters from training)\n",
    "    X_test_processed = preprocessing_pipeline.transform(test_df)\n",
    "\n",
    "    print(\"Training data shape after preprocessing:\", X_train_processed.shape)\n",
    "    print(\"Test data shape after preprocessing:\", X_test_processed.shape)\n",
    "\n",
    "    # You can now use X_train_processed and X_test_processed for modeling"
   ],
   "id": "d13b6e1ea2045fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after preprocessing: (16512, 13)\n",
      "Test data shape after preprocessing: (4128, 13)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:25:39.389428Z",
     "start_time": "2025-09-17T12:25:38.866512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 6 :: Train a Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the original raw data\n",
    "df = pd.read_csv('housing/housing.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('median_house_value', axis=1)  # Features\n",
    "y = df['median_house_value']  # Target\n",
    "\n",
    "# Split the data (using original data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create the full pipeline with Linear Regression\n",
    "full_pipeline = Pipeline([\n",
    "    # Your preprocessing steps\n",
    "    ('bedroom_imputer', BedroomImputer()),\n",
    "\n",
    "    ('column_transformer', ColumnTransformer([\n",
    "        ('numeric', StandardScaler(), numeric_features),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "         categorical_features)\n",
    "    ])),\n",
    "\n",
    "    # Add Linear Regression model\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Linear Regression model...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:,.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Optional: Show some predictions vs actual values\n",
    "print(\"\\nSample predictions vs actual:\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Difference': y_test.values[:10] - y_pred[:10]\n",
    "})\n",
    "print(results_df.round(2))"
   ],
   "id": "2be9e9e37590dbe4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression model...\n",
      "\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 4,907,069,535.27\n",
      "Root Mean Squared Error (RMSE): 70,050.48\n",
      "R² Score: 0.6255\n",
      "\n",
      "Sample predictions vs actual:\n",
      "     Actual  Predicted  Difference\n",
      "0   47700.0   53130.34    -5430.34\n",
      "1   45800.0  123300.23   -77500.23\n",
      "2  500001.0  254769.85   245231.15\n",
      "3  218600.0  268413.59   -49813.59\n",
      "4  278000.0  265647.54    12352.46\n",
      "5  158700.0  138681.20    20018.80\n",
      "6  198200.0  291076.58   -92876.58\n",
      "7  157500.0  228676.03   -71176.03\n",
      "8  340000.0  255787.26    84212.74\n",
      "9  446600.0  408335.02    38264.98\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:33:16.459939Z",
     "start_time": "2025-09-17T12:33:16.440214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 7 : Interpret the results\n",
    "\n",
    "# Interpretation of Results:\n",
    "# 1. Overall Model Performance:\n",
    "# R² Score: 0.6255 - This means your model explains 62.55% of the variance in housing prices. This is actually quite good for a linear regression on real estate data, where many factors influence prices.\n",
    "#\n",
    "# RMSE: $70,050 - On average, your predictions are off by about $70,000 from the actual prices.\n",
    "#\n",
    "# 2. What These Numbers Mean in Context:\n",
    "# Good news: The model has learned meaningful patterns (62.55% explained variance is respectable)\n",
    "#\n",
    "# Reality check: Housing prices are complex! The $70K average error shows there are factors not captured by your features\n",
    "#\n",
    "# 3. Sample Predictions Analysis:\n",
    "# Looking at your sample predictions:\n",
    "#\n",
    "# Good Predictions:\n",
    "#\n",
    "# Row 4: Predicted $265,647 vs Actual $278,000 (only $12,352 off - 4.4% error)\n",
    "#\n",
    "# Row 9: Predicted $408,335 vs Actual $446,600 ($38,265 off - 8.6% error)\n",
    "#\n",
    "# Row 5: Predicted $138,681 vs Actual $158,700 ($20,019 off - 12.6% error)\n",
    "#\n",
    "# Problematic Predictions:\n",
    "#\n",
    "# Row 1: Predicted $123,300 vs Actual $45,800 (overestimated by 169%)\n",
    "#\n",
    "# Row 6: Predicted $291,077 vs Actual $198,200 (overestimated by 47%)\n",
    "#\n",
    "# Row 2: Predicted $254,770 vs Actual $500,001 (underestimated by 49%)\n",
    "#\n",
    "# 4. Why Some Predictions Are So Far Off:\n",
    "# Extreme values: California housing has very high price variations\n",
    "#\n",
    "# Non-linear relationships: Linear regression assumes straight-line relationships, but housing prices often have complex, non-linear patterns\n",
    "#\n",
    "# Missing features: Factors like school quality, crime rates, proximity to amenities aren't in your dataset\n",
    "\n",
    "# 6. Business Interpretation:\n",
    "# Your model is good enough for initial estimates but shouldn't be used for precise valuations. It could be useful for:\n",
    "#\n",
    "# Quick market analysis\n",
    "#\n",
    "# Identifying undervalued/overvalued properties (large differences like row 2)\n",
    "#\n",
    "# Understanding which factors most influence prices\n",
    "#\n",
    "# Conclusion: Solid first model! The 62.55% R² is respectable, but the high RMSE and some large errors suggest either need for better algorithms or more features."
   ],
   "id": "996fe51e5a4deea2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#QUESTION 8 ::: Calculate the Root-Mean-Squared-Error of your model\n",
    "\n",
    "#RESULTS FROM QUESTION 6, the MRSE is already calculated\n",
    "\n",
    "# Model Evaluation:\n",
    "# Mean Squared Error (MSE): 4,907,069,535.27\n",
    "# Root Mean Squared Error (RMSE): 70,050.48\n",
    "# R² Score: 0.6255\n",
    "\n",
    "#\n",
    "# Your RMSE Result:\n",
    "# RMSE = $70,050.48\n",
    "#\n",
    "# How This Was Calculated:\n",
    "# From your output:\n",
    "#\n",
    "# MSE = 4,907,069,535.27 (Mean Squared Error)\n",
    "#\n",
    "# RMSE = √MSE = √4,907,069,535.27 = $70,050.48\n",
    "#\n",
    "# What This RMSE Means:\n",
    "# On average, your predictions are off by approximately $70,050 from the actual housing prices\n",
    "#\n",
    "# This is calculated as: RMSE = sqrt(mean((actual - predicted)²))\n"
   ],
   "id": "3aa7a5b069dad621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:39:50.342699Z",
     "start_time": "2025-09-17T12:39:46.315289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 9 : Compare your previous model with a Decision Tree Regression model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create both pipelines\n",
    "linear_pipeline = Pipeline([\n",
    "    ('bedroom_imputer', BedroomImputer()),\n",
    "    ('column_transformer', ColumnTransformer([\n",
    "        ('numeric', StandardScaler(), numeric_features),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "         categorical_features)\n",
    "    ])),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "decision_tree_pipeline = Pipeline([\n",
    "    ('bedroom_imputer', BedroomImputer()),\n",
    "    ('column_transformer', ColumnTransformer([\n",
    "        ('numeric', 'passthrough', numeric_features),  # No scaling needed for trees\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "         categorical_features)\n",
    "    ])),\n",
    "    ('model', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Train and evaluate both models\n",
    "models = {\n",
    "    'Linear Regression': linear_pipeline,\n",
    "    'Decision Tree': decision_tree_pipeline\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Cross-validation for more robust evaluation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train,\n",
    "                                scoring='neg_mean_squared_error', cv=5)\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "    results[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'CV_RMSE': cv_rmse\n",
    "    }\n",
    "\n",
    "    print(f\"RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Cross-Validated RMSE: ${cv_rmse:,.2f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  RMSE: ${metrics['RMSE']:,.2f}\")\n",
    "    print(f\"  R²: {metrics['R²']:.4f}\")\n",
    "    print(f\"  CV RMSE: ${metrics['CV_RMSE']:,.2f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Expected Results Analysis:\n",
    "# Linear Regression (Your Previous Model):\n",
    "# RMSE: ~$70,050\n",
    "#\n",
    "# R²: ~0.625\n",
    "#\n",
    "# Pros: Simple, interpretable, good baseline\n",
    "#\n",
    "# Cons: Assumes linear relationships, may underfit complex patterns\n",
    "#\n",
    "# Decision Tree (New Model):\n",
    "# Likely Results:\n",
    "#\n",
    "# Training RMSE: Very low ($0-20,000) - trees can overfit!\n",
    "#\n",
    "# Test RMSE: Probably similar or slightly better than linear regression\n",
    "#\n",
    "# R²: May be slightly higher (0.65-0.75)"
   ],
   "id": "971db4818074e3bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Linear Regression ===\n",
      "RMSE: $70,050.48\n",
      "R² Score: 0.6255\n",
      "Cross-Validated RMSE: $68,637.62\n",
      "\n",
      "=== Decision Tree ===\n",
      "RMSE: $69,133.85\n",
      "R² Score: 0.6353\n",
      "Cross-Validated RMSE: $69,759.16\n",
      "\n",
      "==================================================\n",
      "MODEL COMPARISON\n",
      "==================================================\n",
      "Linear Regression:\n",
      "  RMSE: $70,050.48\n",
      "  R²: 0.6255\n",
      "  CV RMSE: $68,637.62\n",
      "\n",
      "Decision Tree:\n",
      "  RMSE: $69,133.85\n",
      "  R²: 0.6353\n",
      "  CV RMSE: $69,759.16\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#QUESTION 10:::Evaluate your model\n",
    "\n",
    "# 1. Your Linear Regression Results:\n",
    "# Your actual results\n",
    "linear_results = {\n",
    "    'RMSE': 70050.48,\n",
    "    'MSE': 4907069535.27,\n",
    "    'R²': 0.6255\n",
    "}\n",
    "\n",
    "\n",
    "# 2. Expected Decision Tree Results (Typical Pattern):\n",
    "\n",
    "# Typical decision tree performance on this dataset\n",
    "decision_tree_results = {\n",
    "    'RMSE': 65000 - 75000,  # Similar or slightly better than linear regression\n",
    "    'R²': 0.65 - 0.70,      # Slight improvement\n",
    "    'Training_RMSE': 10000 - 20000  # Much lower - indicates overfitting!\n",
    "}\n",
    "\n",
    "# Performance Evaluation Metrics\n",
    "# Absolute Error Analysis:\n",
    "\n",
    "# Calculate mean absolute error and percentage error\n",
    "mean_price = y_test.mean()\n",
    "print(f\"Average house price: ${mean_price:,.2f}\")\n",
    "print(f\"Linear Regression RMSE: ${linear_results['RMSE']:,.2f}\")\n",
    "print(f\"Average error percentage: {(linear_results['RMSE']/mean_price)*100:.1f}%\")\n",
    "\n",
    "# Model Assessment\n",
    "# Linear Regression Strengths:\n",
    "# ✅ Interpretable: Coefficients show feature importance\n",
    "#\n",
    "# ✅ Robust: Less prone to overfitting\n",
    "#\n",
    "# ✅ Good baseline: 62.55% variance explained is respectable\n",
    "#\n",
    "# Linear Regression Weaknesses:\n",
    "# ❌ Non-linear relationships: Housing data has complex patterns\n",
    "#\n",
    "# ❌ $70K average error: Too high for precise valuations\n",
    "#\n",
    "# ❌ Assumes linearity: Real estate markets don't work linearly\n",
    "\n",
    "# Decision Tree Likely Performance:\n",
    "# ✅ Better with non-linear patterns: May capture complex relationships\n",
    "#\n",
    "# ✅ Potential R² improvement: Could reach 0.65-0.70\n",
    "#\n",
    "# ❌ Overfitting risk: Great on training, worse on test data\n",
    "#\n",
    "# ❌ Less interpretable: Harder to explain to stakeholders\n",
    "\n",
    "# Final Verdict: Your Linear Regression model is a good baseline (C+ grade) but needs improvement for production use. Decision Tree may slightly outperform it but requires careful tuning to avoid overfitting.\n"
   ],
   "id": "2e80469b5d375694"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
