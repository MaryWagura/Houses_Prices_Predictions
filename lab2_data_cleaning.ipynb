{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-17T11:43:13.724249Z",
     "start_time": "2025-09-17T11:43:13.061576Z"
    }
   },
   "source": [
    "#QUESTION 1 & 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('housing/housing.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values in total_bedrooms\n",
    "# Since total_bedrooms is correlated with households and total_rooms,\n",
    "# we can use median imputation by grouping similar properties\n",
    "\n",
    "# Calculate median bedrooms per household for each ocean_proximity category\n",
    "bedroom_median_by_location = df.groupby('ocean_proximity')['total_bedrooms'].median()\n",
    "\n",
    "# Impute missing values\n",
    "df['total_bedrooms'] = df.apply(\n",
    "    lambda row: bedroom_median_by_location[row['ocean_proximity']]\n",
    "    if pd.isna(row['total_bedrooms'])\n",
    "    else row['total_bedrooms'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Verify no more missing values\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('housing/cleaned_housing_data.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved as 'cleaned_housing_data.csv'\")\n",
    "# Alternative approach: Simple median imputation (if you prefer)\n",
    "# df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "longitude               0\n",
      "latitude                0\n",
      "housing_median_age      0\n",
      "total_rooms             0\n",
      "total_bedrooms        207\n",
      "population              0\n",
      "households              0\n",
      "median_income           0\n",
      "median_house_value      0\n",
      "ocean_proximity         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "longitude             0\n",
      "latitude              0\n",
      "housing_median_age    0\n",
      "total_rooms           0\n",
      "total_bedrooms        0\n",
      "population            0\n",
      "households            0\n",
      "median_income         0\n",
      "median_house_value    0\n",
      "ocean_proximity       0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset saved as 'cleaned_housing_data.csv'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:07:21.799055Z",
     "start_time": "2025-09-17T12:07:21.627184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 4, HANDLE THE NON-NUMERICAL FIELD\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# One-hot encoding with pandas\n",
    "ohe_df = pd.get_dummies(df['ocean_proximity'], prefix='ocean')\n",
    "\n",
    "# Concatenate with original dataframe\n",
    "df_encoded = pd.concat([df.drop('ocean_proximity', axis=1), ohe_df], axis=1)\n",
    "\n",
    "print(\"One-Hot Encoding Results:\")\n",
    "print(ohe_df.head())\n",
    "print(f\"\\nNew columns: {list(ohe_df.columns)}\")"
   ],
   "id": "3b7a4ae6f261d697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding Results:\n",
      "   ocean_<1H OCEAN  ocean_INLAND  ocean_ISLAND  ocean_NEAR BAY  \\\n",
      "0            False         False         False            True   \n",
      "1            False         False         False            True   \n",
      "2            False         False         False            True   \n",
      "3            False         False         False            True   \n",
      "4            False         False         False            True   \n",
      "\n",
      "   ocean_NEAR OCEAN  \n",
      "0             False  \n",
      "1             False  \n",
      "2             False  \n",
      "3             False  \n",
      "4             False  \n",
      "\n",
      "New columns: ['ocean_<1H OCEAN', 'ocean_INLAND', 'ocean_ISLAND', 'ocean_NEAR BAY', 'ocean_NEAR OCEAN']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:26:45.754468Z",
     "start_time": "2025-09-17T12:26:45.379646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 5 :: Use Scikit-learn Pipeline class for transformation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Custom transformer for your specific bedroom imputation strategy\n",
    "class BedroomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.bedroom_medians_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate median bedrooms by ocean_proximity during fitting\n",
    "        self.bedroom_medians_ = X.groupby('ocean_proximity')['total_bedrooms'].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # Apply the same imputation logic you used\n",
    "        X_copy['total_bedrooms'] = X_copy.apply(\n",
    "            lambda row: self.bedroom_medians_[row['ocean_proximity']]\n",
    "            if pd.isna(row['total_bedrooms'])\n",
    "            else row['total_bedrooms'],\n",
    "            axis=1\n",
    "        )\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# Define numeric and categorical features\n",
    "numeric_features = ['longitude', 'latitude', 'housing_median_age',\n",
    "                    'total_rooms', 'total_bedrooms', 'population',\n",
    "                    'households', 'median_income']\n",
    "categorical_features = ['ocean_proximity']\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    # Your custom bedroom imputation\n",
    "    ('bedroom_imputer', BedroomImputer()),\n",
    "\n",
    "    # Column transformer for different data types\n",
    "    ('column_transformer', ColumnTransformer([\n",
    "        # Numeric features: scale them\n",
    "        ('numeric', StandardScaler(), numeric_features),\n",
    "\n",
    "        # Categorical features: one-hot encode\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "         categorical_features)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Example of how to use the pipeline:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('housing/housing.csv')\n",
    "\n",
    "    # Split data (you'll want to do this properly)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train_processed = preprocessing_pipeline.fit_transform(train_df)\n",
    "\n",
    "    # Transform the test data (using learned parameters from training)\n",
    "    X_test_processed = preprocessing_pipeline.transform(test_df)\n",
    "\n",
    "    print(\"Training data shape after preprocessing:\", X_train_processed.shape)\n",
    "    print(\"Test data shape after preprocessing:\", X_test_processed.shape)\n",
    "\n",
    "    # You can now use X_train_processed and X_test_processed for modeling"
   ],
   "id": "d13b6e1ea2045fff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after preprocessing: (16512, 13)\n",
      "Test data shape after preprocessing: (4128, 13)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:25:39.389428Z",
     "start_time": "2025-09-17T12:25:38.866512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#QUESTION 6 :: Train a Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the original raw data\n",
    "df = pd.read_csv('housing/housing.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('median_house_value', axis=1)  # Features\n",
    "y = df['median_house_value']  # Target\n",
    "\n",
    "# Split the data (using original data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create the full pipeline with Linear Regression\n",
    "full_pipeline = Pipeline([\n",
    "    # Your preprocessing steps\n",
    "    ('bedroom_imputer', BedroomImputer()),\n",
    "\n",
    "    ('column_transformer', ColumnTransformer([\n",
    "        ('numeric', StandardScaler(), numeric_features),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "         categorical_features)\n",
    "    ])),\n",
    "\n",
    "    # Add Linear Regression model\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Linear Regression model...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:,.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Optional: Show some predictions vs actual values\n",
    "print(\"\\nSample predictions vs actual:\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Difference': y_test.values[:10] - y_pred[:10]\n",
    "})\n",
    "print(results_df.round(2))"
   ],
   "id": "2be9e9e37590dbe4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression model...\n",
      "\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 4,907,069,535.27\n",
      "Root Mean Squared Error (RMSE): 70,050.48\n",
      "R² Score: 0.6255\n",
      "\n",
      "Sample predictions vs actual:\n",
      "     Actual  Predicted  Difference\n",
      "0   47700.0   53130.34    -5430.34\n",
      "1   45800.0  123300.23   -77500.23\n",
      "2  500001.0  254769.85   245231.15\n",
      "3  218600.0  268413.59   -49813.59\n",
      "4  278000.0  265647.54    12352.46\n",
      "5  158700.0  138681.20    20018.80\n",
      "6  198200.0  291076.58   -92876.58\n",
      "7  157500.0  228676.03   -71176.03\n",
      "8  340000.0  255787.26    84212.74\n",
      "9  446600.0  408335.02    38264.98\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
