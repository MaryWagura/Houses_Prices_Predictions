{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f163a68c",
   "metadata": {},
   "source": [
    "Calling the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for SVM implementation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7cbbd",
   "metadata": {},
   "source": [
    "Fetch the Iris Dataset\n",
    "Why the Iris dataset?\n",
    "\n",
    "Classic dataset for classification\n",
    "\n",
    "Well-balanced classes\n",
    "\n",
    "Good for demonstrating SVM capabilities\n",
    "\n",
    "Small enough for quick experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba691ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", iris.target_names)\n",
    "print(\"Dataset shape:\", iris.data.shape)\n",
    "\n",
    "# Let's examine the data structure\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Targets: 0=setosa, 1=versicolor, 2=virginica\n",
    "\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "print(X[:5])\n",
    "print(\"Corresponding labels:\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e869e",
   "metadata": {},
   "source": [
    "Linear SVM Classification\n",
    "Why Linear SVM?\n",
    "\n",
    "Finds optimal hyperplane that maximizes margin between classes\n",
    "\n",
    "Works well when data is linearly separable\n",
    "\n",
    "C parameter controls trade-off between margin width and classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll focus on two classes for binary classification (setosa vs versicolor)\n",
    "X_binary = iris.data[:, :2]  # Using only sepal length and width for visualization\n",
    "y_binary = (iris.target != 0) * 1  # Convert to binary: setosa(0) vs non-setosa(1)\n",
    "\n",
    "# Create and train linear SVM\n",
    "linear_svm = SVC(kernel='linear', C=1.0)\n",
    "linear_svm.fit(X_binary, y_binary)\n",
    "\n",
    "# Create mesh for decision boundary visualization\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(linear_svm, X_binary, y_binary, \"Linear SVM Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad92c83",
   "metadata": {},
   "source": [
    "Polynomial Kernels\n",
    "Why Polynomial Kernels?\n",
    "\n",
    "Handle non-linearly separable data\n",
    "\n",
    "degree parameter controls complexity of decision boundary\n",
    "\n",
    "Higher degrees = more complex boundaries (risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel SVM\n",
    "poly_svm = SVC(kernel='poly', degree=3, C=1.0, coef0=1)\n",
    "poly_svm.fit(X_binary, y_binary)\n",
    "\n",
    "plot_decision_boundary(poly_svm, X_binary, y_binary, \"Polynomial Kernel SVM (degree=3)\")\n",
    "\n",
    "# Compare different polynomial degrees\n",
    "degrees = [2, 3, 4]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    poly_svm = SVC(kernel='poly', degree=degree, C=1.0)\n",
    "    poly_svm.fit(X_binary, y_binary)\n",
    "    \n",
    "    # Plotting code similar to above for each subplot\n",
    "    x_min, x_max = X_binary[:, 0].min() - 1, X_binary[:, 0].max() + 1\n",
    "    y_min, y_max = X_binary[:, 1].min() - 1, X_binary[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = poly_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "    axes[idx].scatter(X_binary[:, 0], X_binary[:, 1], c=y_binary, edgecolors='k')\n",
    "    axes[idx].set_title(f'Polynomial degree={degree}')\n",
    "    axes[idx].set_xlabel('Sepal length')\n",
    "    axes[idx].set_ylabel('Sepal width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0db65",
   "metadata": {},
   "source": [
    " Gaussian Kernel (RBF)\n",
    " Why Gaussian (RBF) Kernel?\n",
    "\n",
    "Most commonly used kernel\n",
    "\n",
    "gamma controls influence of individual training examples\n",
    "\n",
    "Low gamma = smooth decision boundary\n",
    "\n",
    "High gamma = complex, wiggly boundary (risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF (Gaussian) kernel SVM\n",
    "rbf_svm = SVC(kernel='rbf', gamma=0.7, C=1.0)\n",
    "rbf_svm.fit(X_binary, y_binary)\n",
    "\n",
    "plot_decision_boundary(rbf_svm, X_binary, y_binary, \"RBF Kernel SVM\")\n",
    "\n",
    "# Compare different gamma values\n",
    "gammas = [0.1, 1, 10]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    rbf_svm = SVC(kernel='rbf', gamma=gamma, C=1.0)\n",
    "    rbf_svm.fit(X_binary, y_binary)\n",
    "    \n",
    "    # Similar plotting code as before\n",
    "    x_min, x_max = X_binary[:, 0].min() - 1, X_binary[:, 0].max() + 1\n",
    "    y_min, y_max = X_binary[:, 1].min() - 1, X_binary[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = rbf_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "    axes[idx].scatter(X_binary[:, 0], X_binary[:, 1], c=y_binary, edgecolors='k')\n",
    "    axes[idx].set_title(f'RBF gamma={gamma}')\n",
    "    axes[idx].set_xlabel('Sepal length')\n",
    "    axes[idx].set_ylabel('Sepal width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e65f29",
   "metadata": {},
   "source": [
    "SVM Regression\n",
    "Why SVM Regression?\n",
    "\n",
    "Finds a function that deviates from training data by at most Îµ\n",
    "\n",
    "Tries to fit as many points as possible within a margin\n",
    "\n",
    "Useful when you want to ignore small errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for regression\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X_reg = 2 * np.random.rand(m, 1) - 1\n",
    "y_reg = (0.2 + 0.1 * X_reg + 0.5 * X_reg**2 + np.random.randn(m, 1)/10).ravel()\n",
    "\n",
    "# SVM Regression with different kernels\n",
    "svm_lin_reg = SVR(kernel=\"linear\", C=100)\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, coef0=1)\n",
    "svm_rbf_reg = SVR(kernel=\"rbf\", C=100, gamma=0.1)\n",
    "\n",
    "# Fit models\n",
    "svm_lin_reg.fit(X_reg, y_reg)\n",
    "svm_poly_reg.fit(X_reg, y_reg)\n",
    "svm_rbf_reg.fit(X_reg, y_reg)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "models = [svm_lin_reg, svm_poly_reg, svm_rbf_reg]\n",
    "titles = ['Linear SVR', 'Polynomial SVR (degree=2)', 'RBF SVR']\n",
    "\n",
    "for idx, (model, title) in enumerate(zip(models, titles)):\n",
    "    X_new = np.linspace(-1, 1, 100).reshape(100, 1)\n",
    "    y_pred = model.predict(X_new)\n",
    "    \n",
    "    axes[idx].plot(X_reg, y_reg, \"b.\")\n",
    "    axes[idx].plot(X_new, y_pred, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "    axes[idx].set_xlabel(\"X\")\n",
    "    axes[idx].set_ylabel(\"y\")\n",
    "    axes[idx].set_title(title)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfc60e",
   "metadata": {},
   "source": [
    "Decision Function and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ec5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use all features for multiclass classification\n",
    "X_full = iris.data\n",
    "y_full = iris.target\n",
    "\n",
    "# Scale the data (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "# Create SVM with RBF kernel for multiclass classification\n",
    "svm_multiclass = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_multiclass.fit(X_scaled, y_full)\n",
    "\n",
    "# Decision function and predictions\n",
    "print(\"=== Decision Function and Predictions ===\")\n",
    "\n",
    "# Get some test samples\n",
    "test_samples = X_scaled[:5]\n",
    "predictions = svm_multiclass.predict(test_samples)\n",
    "decision_scores = svm_multiclass.decision_function(test_samples)\n",
    "\n",
    "print(\"Test samples (first 5):\")\n",
    "print(test_samples)\n",
    "print(\"\\nPredictions:\", predictions)\n",
    "print(\"Actual labels:\", y_full[:5])\n",
    "print(\"\\nDecision function scores (higher = more confident):\")\n",
    "print(decision_scores)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n=== Interpretation ===\")\n",
    "print(\"For each sample, the decision function returns scores for all 3 classes\")\n",
    "print(\"The highest score determines the predicted class\")\n",
    "print(\"Class 0: Setosa, Class 1: Versicolor, Class 2: Virginica\")\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm_multiclass, X_scaled, y_full, cv=5)\n",
    "print(f\"\\nCross-validation accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete analysis\n",
    "def comprehensive_analysis():\n",
    "    print(\"=== COMPREHENSIVE SVM ANALYSIS ON IRIS DATASET ===\\n\")\n",
    "    \n",
    "    # 1. Data Overview\n",
    "    print(\"1. DATASET OVERVIEW\")\n",
    "    print(f\"   Samples: {iris.data.shape[0]}\")\n",
    "    print(f\"   Features: {iris.data.shape[1]}\")\n",
    "    print(f\"   Classes: {len(np.unique(iris.target))}\")\n",
    "    print(f\"   Class distribution: {np.bincount(iris.target)}\")\n",
    "    \n",
    "    # 2. Feature Analysis\n",
    "    print(\"\\n2. FEATURE ANALYSIS\")\n",
    "    feature_names = iris.feature_names\n",
    "    for i, name in enumerate(feature_names):\n",
    "        print(f\"   {name}: min={iris.data[:, i].min():.1f}, max={iris.data[:, i].max():.1f}, \"\n",
    "              f\"mean={iris.data[:, i].mean():.1f}\")\n",
    "    \n",
    "    # 3. Model Comparison\n",
    "    print(\"\\n3. MODEL COMPARISON\")\n",
    "    kernels = ['linear', 'poly', 'rbf']\n",
    "    X_scaled = StandardScaler().fit_transform(iris.data)\n",
    "    \n",
    "    for kernel in kernels:\n",
    "        if kernel == 'poly':\n",
    "            svm = SVC(kernel=kernel, degree=3)\n",
    "        else:\n",
    "            svm = SVC(kernel=kernel)\n",
    "        \n",
    "        scores = cross_val_score(svm, X_scaled, iris.target, cv=5)\n",
    "        print(f\"   {kernel.upper()} kernel: {scores.mean():.3f} accuracy\")\n",
    "    \n",
    "    # 4. Key Insights\n",
    "    print(\"\\n4. KEY INSIGHTS\")\n",
    "    print(\"   - SVMs work best with scaled features\")\n",
    "    print(\"   - RBF kernel often performs well for complex boundaries\")\n",
    "    print(\"   - Linear kernel is efficient for linearly separable data\")\n",
    "    print(\"   - Polynomial kernel can capture polynomial relationships\")\n",
    "    print(\"   - C parameter controls regularization (overfitting vs underfitting)\")\n",
    "    print(\"   - Gamma (RBF) controls influence range of training examples\")\n",
    "\n",
    "# Execute comprehensive analysis\n",
    "comprehensive_analysis()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
