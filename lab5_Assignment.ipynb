{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train an SVM Classifier on the MNIST Dataset",
   "id": "5918c79b152e94c9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T17:25:10.487253Z",
     "start_time": "2025-10-10T17:19:48.629259Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "print(\"Loading MNIST dataset... (This may take a moment and requires internet access)\")\n",
    "# Fetch the full MNIST dataset (70,000 images, 784 features)\n",
    "# 'mnist_784' is the dataset name on OpenML\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# Features (X) are the flattened images (784 pixels)\n",
    "X = mnist.data.astype('float32')\n",
    "# Labels (y) are the digits (0-9)\n",
    "y = mnist.target.astype('int')\n",
    "\n",
    "print(f\"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features.\")\n",
    "\n",
    "# --- 2. Data Preprocessing and Splitting ---\n",
    "\n",
    "# Normalize the data: scale pixel values from [0, 255] to [0, 1]\n",
    "# This is crucial for most ML algorithms, especially SVMs.\n",
    "X /= 255.0\n",
    "\n",
    "# Optional: To save significant computation time, you can work with a smaller subset.\n",
    "# Uncomment the following lines to use only the first 10,000 samples.\n",
    "# N_SAMPLES = 10000\n",
    "# print(f\"Using a subset of {N_SAMPLES} samples for faster training.\")\n",
    "# X = X[:N_SAMPLES]\n",
    "# y = y[:N_SAMPLES]\n",
    "\n",
    "# Split the data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# --- 3. Model Training ---\n",
    "\n",
    "# Create an SVM Classifier with a Radial Basis Function (RBF) kernel\n",
    "# RBF kernel is generally a good choice for classification problems like this.\n",
    "# C: Regularization parameter. Higher C means lower tolerance for misclassification.\n",
    "# gamma: Kernel coefficient. 'scale' is a good default: 1 / (n_features * X.var())\n",
    "clf = svm.SVC(gamma='scale', kernel='rbf', C=10)\n",
    "\n",
    "print(\"\\nStarting SVM training... (This may take a long time on a large dataset)\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 4. Model Evaluation ---\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"SVM Classifier Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset... (This may take a moment and requires internet access)\n",
      "Dataset loaded: 70000 samples, 784 features.\n",
      "Training set size: 56000 samples\n",
      "Test set size: 14000 samples\n",
      "\n",
      "Starting SVM training... (This may take a long time on a large dataset)\n",
      "Training finished in 184.25 seconds.\n",
      "\n",
      "--- Model Evaluation ---\n",
      "SVM Classifier Accuracy on Test Set: 98.34%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1381\n",
      "           1       0.99      0.99      0.99      1575\n",
      "           2       0.98      0.98      0.98      1398\n",
      "           3       0.99      0.98      0.98      1428\n",
      "           4       0.98      0.98      0.98      1365\n",
      "           5       0.98      0.98      0.98      1263\n",
      "           6       0.99      0.99      0.99      1375\n",
      "           7       0.98      0.98      0.98      1459\n",
      "           8       0.98      0.98      0.98      1365\n",
      "           9       0.97      0.98      0.98      1391\n",
      "\n",
      "    accuracy                           0.98     14000\n",
      "   macro avg       0.98      0.98      0.98     14000\n",
      "weighted avg       0.98      0.98      0.98     14000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use Grid/Random Search with Cross-Validation to find the best hyperparameter values for the SVM classifier.\n",
    "For the Polynomial Kernel, optimize the degree, C, and coef0 hyperparameters.\n",
    "For the RBF Kernel, focus on optimizing C and gamma."
   ],
   "id": "3bae6765d2e7575a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T17:29:16.944296Z",
     "start_time": "2025-10-10T17:27:00.053919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. Load and Prepare Data (Using a Subset for Efficient Search) ---\n",
    "\n",
    "print(\"Loading MNIST dataset for hyperparameter tuning...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X_full = mnist.data.astype('float32') / 255.0  # Normalize\n",
    "y_full = mnist.target.astype('int')\n",
    "\n",
    "# CRITICAL STEP: Use a small, manageable subset for GridSearchCV.\n",
    "# Grid Search is computationally expensive (O(n^2) to O(n^3)) for SVC.\n",
    "N_SAMPLES = 6000 # Use 6,000 samples for the search (fast enough for testing)\n",
    "X_subset = X_full[:N_SAMPLES]\n",
    "y_subset = y_full[:N_SAMPLES]\n",
    "\n",
    "# Split the subset for the hyperparameter search\n",
    "# Note: We are splitting the subset into 'search_train' and 'search_test'\n",
    "# for the GridSearchCV, which will internally use K-fold cross-validation on 'search_train'.\n",
    "X_train_search, X_test_search, y_train_search, y_test_search = train_test_split(\n",
    "    X_subset, y_subset, test_size=0.2, random_state=42, stratify=y_subset\n",
    ")\n",
    "\n",
    "print(f\"Using a subset of {X_subset.shape[0]} samples for Grid Search.\")\n",
    "print(f\"Grid Search Training Set Size: {X_train_search.shape[0]}\")\n",
    "\n",
    "# --- 2. Grid Search for RBF Kernel ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Grid Search for RBF Kernel (C, gamma)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the parameter grid for the RBF kernel\n",
    "# C: Regularization parameter. Values are typically powers of 10.\n",
    "# gamma: Kernel coefficient. 'scale' is a robust default, but we test others.\n",
    "param_grid_rbf = [\n",
    "    {'C': [1, 10], 'gamma': [0.001, 0.01, 'scale']},\n",
    "]\n",
    "\n",
    "# Create the SVC model\n",
    "svc_rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Create the Grid Search object\n",
    "# cv=3 means 3-fold cross-validation: the training set is split into 3 folds,\n",
    "# and the model is trained 3 times on 2 folds and tested on the remaining fold.\n",
    "grid_search_rbf = GridSearchCV(\n",
    "    svc_rbf, param_grid_rbf, cv=3, verbose=2, n_jobs=-1, scoring='accuracy'\n",
    ")\n",
    "\n",
    "start_time_rbf = time.time()\n",
    "grid_search_rbf.fit(X_train_search, y_train_search)\n",
    "end_time_rbf = time.time()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nRBF Grid Search finished in {end_time_rbf - start_time_rbf:.2f} seconds.\")\n",
    "print(f\"Best RBF parameters found: {grid_search_rbf.best_params_}\")\n",
    "print(f\"Best RBF Cross-Validation score: {grid_search_rbf.best_score_:.4f}\")\n",
    "\n",
    "# Optional: Evaluate on the hold-out test set\n",
    "best_rbf_clf = grid_search_rbf.best_estimator_\n",
    "test_accuracy_rbf = best_rbf_clf.score(X_test_search, y_test_search)\n",
    "print(f\"Best RBF Model Accuracy on Test Subset: {test_accuracy_rbf:.4f}\")\n",
    "\n",
    "# --- 3. Grid Search for Polynomial Kernel ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Grid Search for Polynomial Kernel (degree, C, coef0)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the parameter grid for the Polynomial kernel\n",
    "# degree: Degree of the polynomial kernel function (e.g., degree 3 for cubic)\n",
    "# C: Regularization parameter.\n",
    "# coef0: Independent term in the kernel function.\n",
    "param_grid_poly = [\n",
    "    {\n",
    "        'degree': [2, 3],\n",
    "        'C': [1, 10],\n",
    "        'coef0': [0.0, 1.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the SVC model\n",
    "svc_poly = svm.SVC(kernel='poly')\n",
    "\n",
    "# Create the Grid Search object\n",
    "grid_search_poly = GridSearchCV(\n",
    "    svc_poly, param_grid_poly, cv=3, verbose=2, n_jobs=-1, scoring='accuracy'\n",
    ")\n",
    "\n",
    "start_time_poly = time.time()\n",
    "grid_search_poly.fit(X_train_search, y_train_search)\n",
    "end_time_poly = time.time()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nPolynomial Grid Search finished in {end_time_poly - start_time_poly:.2f} seconds.\")\n",
    "print(f\"Best Polynomial parameters found: {grid_search_poly.best_params_}\")\n",
    "print(f\"Best Polynomial Cross-Validation score: {grid_search_poly.best_score_:.4f}\")\n",
    "\n",
    "# Optional: Evaluate on the hold-out test set\n",
    "best_poly_clf = grid_search_poly.best_estimator_\n",
    "test_accuracy_poly = best_poly_clf.score(X_test_search, y_test_search)\n",
    "print(f\"Best Polynomial Model Accuracy on Test Subset: {test_accuracy_poly:.4f}\")"
   ],
   "id": "c65ce5d8a38861be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset for hyperparameter tuning...\n",
      "Using a subset of 6000 samples for Grid Search.\n",
      "Grid Search Training Set Size: 4800\n",
      "\n",
      "==================================================\n",
      "Starting Grid Search for RBF Kernel (C, gamma)\n",
      "==================================================\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=  17.0s\n",
      "[CV] END ...................................C=1, gamma=0.001; total time=  21.1s\n",
      "[CV] END ...................................C=1, gamma=0.001; total time=  23.0s\n",
      "[CV] END ...................................C=1, gamma=0.001; total time=  24.8s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=  14.8s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=  14.5s\n",
      "[CV] END ...................................C=1, gamma=scale; total time=  12.2s\n",
      "[CV] END ...................................C=1, gamma=scale; total time=  15.7s\n",
      "[CV] END ...................................C=1, gamma=scale; total time=  10.0s\n",
      "[CV] END ..................................C=10, gamma=0.001; total time=   7.7s\n",
      "[CV] END ..................................C=10, gamma=0.001; total time=   9.6s\n",
      "[CV] END ..................................C=10, gamma=0.001; total time=  10.1s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   9.8s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=  10.4s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   9.8s\n",
      "[CV] END ..................................C=10, gamma=scale; total time=  11.5s\n",
      "[CV] END ..................................C=10, gamma=scale; total time=  10.7s\n",
      "[CV] END ..................................C=10, gamma=scale; total time=   9.5s\n",
      "\n",
      "RBF Grid Search finished in 73.84 seconds.\n",
      "Best RBF parameters found: {'C': 10, 'gamma': 'scale'}\n",
      "Best RBF Cross-Validation score: 0.9562\n",
      "Best RBF Model Accuracy on Test Subset: 0.9592\n",
      "\n",
      "==================================================\n",
      "Starting Grid Search for Polynomial Kernel (degree, C, coef0)\n",
      "==================================================\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] END ...........................C=1, coef0=0.0, degree=2; total time=   7.2s\n",
      "[CV] END ...........................C=1, coef0=0.0, degree=2; total time=   8.6s\n",
      "[CV] END ...........................C=1, coef0=0.0, degree=2; total time=   9.5s\n",
      "[CV] END ...........................C=1, coef0=0.0, degree=3; total time=   9.7s\n",
      "[CV] END ...........................C=1, coef0=0.0, degree=3; total time=   7.2s\n",
      "[CV] END ...........................C=1, coef0=1.0, degree=2; total time=   7.2s\n",
      "[CV] END ...........................C=1, coef0=1.0, degree=2; total time=   7.8s\n",
      "[CV] END ...........................C=1, coef0=0.0, degree=3; total time=   9.9s\n",
      "[CV] END ...........................C=1, coef0=1.0, degree=2; total time=   6.0s\n",
      "[CV] END ...........................C=1, coef0=1.0, degree=3; total time=   7.9s\n",
      "[CV] END ...........................C=1, coef0=1.0, degree=3; total time=   7.5s\n",
      "[CV] END ...........................C=1, coef0=1.0, degree=3; total time=   8.1s\n",
      "[CV] END ..........................C=10, coef0=0.0, degree=2; total time=   6.9s\n",
      "[CV] END ..........................C=10, coef0=0.0, degree=2; total time=   5.4s\n",
      "[CV] END ..........................C=10, coef0=0.0, degree=2; total time=   7.7s\n",
      "[CV] END ..........................C=10, coef0=0.0, degree=3; total time=   7.7s\n",
      "[CV] END ..........................C=10, coef0=0.0, degree=3; total time=   8.0s\n",
      "[CV] END ..........................C=10, coef0=0.0, degree=3; total time=   7.2s\n",
      "[CV] END ..........................C=10, coef0=1.0, degree=2; total time=   6.6s\n",
      "[CV] END ..........................C=10, coef0=1.0, degree=2; total time=   6.3s\n",
      "[CV] END ..........................C=10, coef0=1.0, degree=2; total time=   6.6s\n",
      "[CV] END ..........................C=10, coef0=1.0, degree=3; total time=   6.8s\n",
      "[CV] END ..........................C=10, coef0=1.0, degree=3; total time=   5.8s\n",
      "[CV] END ..........................C=10, coef0=1.0, degree=3; total time=   5.2s\n",
      "\n",
      "Polynomial Grid Search finished in 48.70 seconds.\n",
      "Best Polynomial parameters found: {'C': 10, 'coef0': 0.0, 'degree': 2}\n",
      "Best Polynomial Cross-Validation score: 0.9452\n",
      "Best Polynomial Model Accuracy on Test Subset: 0.9558\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare the performance of the SVM with different kernels (Linear, Polynomial, and RBF) and select the best one based on test set accuracy and other metrics like precision, recall, and F1-score.",
   "id": "9256560886a4903a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Part A: Train and Evaluate Linear Kernel SVM",
   "id": "fb876c97af196065"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T17:39:51.496650Z",
     "start_time": "2025-10-10T17:34:12.055870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "# --- 1. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"Loading MNIST dataset for Linear SVM...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X = mnist.data.astype('float32') / 255.0  # Normalize\n",
    "y = mnist.target.astype('int')\n",
    "\n",
    "# Split the full data (same split as Task 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
    ")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# --- 2. Train Linear Kernel SVM ---\n",
    "# Use the default C=1.0 for the Linear Kernel SVC\n",
    "print(\"\\nStarting SVM training with LINEAR Kernel...\")\n",
    "clf_linear = svm.SVC(kernel='linear', C=1.0)\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the classifier\n",
    "clf_linear.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 3. Model Evaluation ---\n",
    "y_pred_linear = clf_linear.predict(X_test)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "print(\"\\n--- Linear Kernel Model Evaluation ---\")\n",
    "print(f\"Linear SVM Classifier Accuracy on Test Set: {accuracy_linear * 100:.2f}%\")\n",
    "\n",
    "# Print detailed classification report\n",
    "report_linear = classification_report(y_test, y_pred_linear, output_dict=True)\n",
    "print(\"\\nClassification Report (Linear Kernel):\")\n",
    "print(classification_report(y_test, y_pred_linear))"
   ],
   "id": "54ecf0b91b39f8e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset for Linear SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x7dffcc382660>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clauds/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/home/clauds/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/home/clauds/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x742a5e17e660>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clauds/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/home/clauds/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/home/clauds/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 56000 samples\n",
      "Test set size: 14000 samples\n",
      "\n",
      "Starting SVM training with LINEAR Kernel...\n",
      "Training finished in 244.00 seconds.\n",
      "\n",
      "--- Linear Kernel Model Evaluation ---\n",
      "Linear SVM Classifier Accuracy on Test Set: 93.59%\n",
      "\n",
      "Classification Report (Linear Kernel):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1381\n",
      "           1       0.96      0.98      0.97      1575\n",
      "           2       0.92      0.93      0.93      1398\n",
      "           3       0.90      0.92      0.91      1428\n",
      "           4       0.94      0.94      0.94      1365\n",
      "           5       0.92      0.90      0.91      1263\n",
      "           6       0.96      0.96      0.96      1375\n",
      "           7       0.95      0.95      0.95      1459\n",
      "           8       0.93      0.89      0.91      1365\n",
      "           9       0.94      0.90      0.92      1391\n",
      "\n",
      "    accuracy                           0.94     14000\n",
      "   macro avg       0.94      0.93      0.93     14000\n",
      "weighted avg       0.94      0.94      0.94     14000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare the SVM classifier’s performance with your classifiers from Assignment 4, i.e KNN, SGD, and Random Forest. Pay attention to accuracy, precision, recall, and other evaluation metrics. Also, include training time (computational complexity) as evaluation metric.",
   "id": "2a8408cf1a63f1d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Final Conclusion and Trade-Off\n",
    "The comparison highlights the classic accuracy vs. speed trade-off in machine learning:\n",
    "\n",
    "The SVM with RBF Kernel is the best classifier in terms of pure performance (≈98.34% accuracy) but has the highest computational cost during training, making it the slowest choice.\n",
    "\n",
    "The SGD Classifier is the best classifier in terms of speed and scalability, offering near-instantaneous training at the expense of a significant drop in accuracy (≈91.39%)."
   ],
   "id": "5eeeb6d1402ab8c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Choice\tBest For...\n",
    "\n",
    "SVM (RBF)\tMaximum Accuracy: When high performance is paramount and training time/resources are not a major constraint.\n",
    "\n",
    "KNN\tFast Training: When a high-accuracy, simple model is needed, and you can tolerate a slow prediction time.\n",
    "\n",
    "SGD\tSpeed and Scalability: When data size is enormous or training must be done quickly (e.g., streaming data), even if accuracy is sacrificed."
   ],
   "id": "8e8b5e8df335d730"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
